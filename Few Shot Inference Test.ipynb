{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/opthollm/MiniGPT-4\n"
     ]
    }
   ],
   "source": [
    "%cd MiniGPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Packages\n",
    "Import minigpt4 and necessary helper libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import gradio as gr\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.dist_utils import get_rank\n",
    "from minigpt4.common.registry import registry\n",
    "from minigpt4.conversation.multi_img_conversation import Chat, CONV_VISION\n",
    "\n",
    "# imports modules for registration\n",
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *\n",
    "import os\n",
    "\n",
    "import argparse as argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods\n",
    "Define helper methods including encode diagnosis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Methods\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "    parser.add_argument(\"--cfg-path\", required=True, help=\"path to configuration file.\")\n",
    "    parser.add_argument(\"--gpu-id\", type=int, default=0, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--num-beams\", type=int, default=2, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--temperature\", type=int, default=0.9, help=\"specify the gpu to load the model.\")\n",
    "    parser.add_argument(\"--english\", type=bool, default=True, help=\"chinese or english\")\n",
    "    parser.add_argument(\"--prompt-en\", type=str, default=\"can you describe the current picture?\", help=\"Can you describe the current picture?\")\n",
    "    parser.add_argument(\"--prompt-zh\", type=str, default=\"你能描述一下当前的图片？\", help=\"Can you describe the current picture?\")\n",
    "    parser.add_argument(\n",
    "        \"--options\",\n",
    "        nargs=\"+\",\n",
    "        help=\"override some settings in the used config, the key-value pair \"\n",
    "        \"in xxx=yyy format will be merged into config file (deprecate), \"\n",
    "        \"change to --cfg-options instead.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def setup_seeds(config):\n",
    "    seed = config.run_cfg.seed + get_rank()\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "\n",
    "### fix this method since it's not completely accurate \n",
    "\n",
    "# determines if the LLM thinks the image is glaucomatous or not based on whether or not the text contains glaucomatous or not \n",
    "def encode_diagnosis(diagnosis):\n",
    "    # could add: if contains glaucomatous and normal, then only look at first sentence\n",
    "    \n",
    "    if 'glaucomatous' in diagnosis.lower():\n",
    "        return 1\n",
    "    if 'normal' in diagnosis.lower():\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# finds the true label of an image based on where it's stored in file path \n",
    "def fetch_ground_truth(img_path):\n",
    "    split_string = img_path.split(\"/\")\n",
    "\n",
    "    # Find the index of \"glaucoma\" in the split string\n",
    "    try:\n",
    "        split_string.index(\"glaucoma\")\n",
    "        return 1\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# helper method that gets all files from a directory \n",
    "def get_all_files(directory):\n",
    "    all_files = []\n",
    "    \n",
    "    # Iterate over all the directories and files within the given directory\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            all_files.append(file_path)\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "def get_random_file(directory):\n",
    "    all_files = get_all_files(directory)\n",
    "    return random.choice(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chat\n",
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da892bf72d3346d7988528544753684e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n",
      "Load 4 training prompts\n",
      "Prompt Example \n",
      "###Human: <Img><ImageHere></Img> Could you describe the contents of this image for me? ###Assistant: \n",
      "Load BLIP2-LLM Checkpoint: pretrained_minigpt4.pth\n",
      "Initialization Finished\n",
      "Intializing Test\n"
     ]
    }
   ],
   "source": [
    "print('Initializing Chat')\n",
    "#args = parse_args()\n",
    "#args = preset_args\n",
    "args = argparse.Namespace(cfg_path='eval_configs/minigpt4_eval.yaml', gpu_id=0, num_beams=2, temperature=0.9, english=True, prompt_en='can you describe the current picture?', prompt_zh='你能描述一下当前的图片？', options=None)\n",
    "cfg = Config(args)\n",
    "\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "chat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id))\n",
    "print('Initialization Finished')\n",
    "\n",
    "print('Intializing Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Few Shot Learning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store output\n",
    "\n",
    "few_shot_data = {'img_path:': [],\n",
    "                 'prediction:': [],\n",
    "                 'ground_truth:': [],\n",
    "                 'llm_message': []\n",
    "                 }\n",
    "\n",
    "# define examples for few shot learning\n",
    "\n",
    "from chain_of_thought_imgs import img_descriptions\n",
    "examples = img_descriptions.chain_of_thought_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">42</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">39 </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">40 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41 # have the model answer and display </span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>42 llm_message = llm_message = chat.answer(                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">43 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>conv=chat_state,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>img_list=img_list,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>num_beams=args.num_beams,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jupyter/opthollm/MiniGPT-4/minigpt4/conversation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">multi_img_conversation.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">149</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">answer</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">146 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">answer</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, conv, img_list, max_new_tokens=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">300</span>, num_beams=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, min_length=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, top_   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">147 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │      </span>repetition_penalty=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1.0</span>, length_penalty=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, temperature=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1.0</span>, max_length=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">148 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>conv.append_message(conv.roles[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>149 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>embs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.get_context_emb(conv, img_list)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">150 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">151 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>current_max_len = embs.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] + max_new_tokens                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">152 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> current_max_len - max_length &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jupyter/opthollm/MiniGPT-4/minigpt4/conversation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">multi_img_conversation.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">224</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_context_emb</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">221 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_context_emb</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, conv, img_list):                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">222 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>prompt = conv.get_prompt()                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">223 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>prompt_segs = prompt.split(<span style=\"color: #808000; text-decoration-color: #808000\">'&lt;ImageHere&gt;'</span>)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>224 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(prompt_segs) == <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(img_list) + <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"Unmatched numbers of image placeh</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">225 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>seg_tokens = [                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">226 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model.llama_tokenizer(                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">227 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>seg, return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>, add_special_tokens=i == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>).to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device).inp   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AssertionError: </span>Unmatched numbers of image placeholders and images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m42\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m39 \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m40 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m41 \u001b[0m\u001b[2m# have the model answer and display \u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m42 llm_message = llm_message = chat.answer(                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m43 \u001b[0m\u001b[2m│   │   \u001b[0mconv=chat_state,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m44 \u001b[0m\u001b[2m│   │   \u001b[0mimg_list=img_list,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m45 \u001b[0m\u001b[2m│   │   \u001b[0mnum_beams=args.num_beams,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jupyter/opthollm/MiniGPT-4/minigpt4/conversation/\u001b[0m\u001b[1;33mmulti_img_conversation.py\u001b[0m:\u001b[94m149\u001b[0m in \u001b[92manswer\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m146 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92manswer\u001b[0m(\u001b[96mself\u001b[0m, conv, img_list, max_new_tokens=\u001b[94m300\u001b[0m, num_beams=\u001b[94m1\u001b[0m, min_length=\u001b[94m1\u001b[0m, top_   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m147 \u001b[0m\u001b[2m│   │   │      \u001b[0mrepetition_penalty=\u001b[94m1.0\u001b[0m, length_penalty=\u001b[94m1\u001b[0m, temperature=\u001b[94m1.0\u001b[0m, max_length=\u001b[94m200\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m148 \u001b[0m\u001b[2m│   │   \u001b[0mconv.append_message(conv.roles[\u001b[94m1\u001b[0m], \u001b[94mNone\u001b[0m)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m149 \u001b[2m│   │   \u001b[0membs = \u001b[96mself\u001b[0m.get_context_emb(conv, img_list)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m150 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   \u001b[0mcurrent_max_len = embs.shape[\u001b[94m1\u001b[0m] + max_new_tokens                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m current_max_len - max_length > \u001b[94m0\u001b[0m:                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/jupyter/opthollm/MiniGPT-4/minigpt4/conversation/\u001b[0m\u001b[1;33mmulti_img_conversation.py\u001b[0m:\u001b[94m224\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mget_context_emb\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mget_context_emb\u001b[0m(\u001b[96mself\u001b[0m, conv, img_list):                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m│   │   \u001b[0mprompt = conv.get_prompt()                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m223 \u001b[0m\u001b[2m│   │   \u001b[0mprompt_segs = prompt.split(\u001b[33m'\u001b[0m\u001b[33m<ImageHere>\u001b[0m\u001b[33m'\u001b[0m)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m224 \u001b[2m│   │   \u001b[0m\u001b[94massert\u001b[0m \u001b[96mlen\u001b[0m(prompt_segs) == \u001b[96mlen\u001b[0m(img_list) + \u001b[94m1\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mUnmatched numbers of image placeh\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m225 \u001b[0m\u001b[2m│   │   \u001b[0mseg_tokens = [                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.model.llama_tokenizer(                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m227 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mseg, return_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m, add_special_tokens=i == \u001b[94m0\u001b[0m).to(\u001b[96mself\u001b[0m.device).inp   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAssertionError: \u001b[0mUnmatched numbers of image placeholders and images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directory = 'RIM-ONE_DL_images/partitioned_randomly/training_set'\n",
    "# pick random training image to test on\n",
    "\n",
    "img_list = []\n",
    "chat_state = CONV_VISION.copy()\n",
    "\n",
    "image = get_random_file(directory)\n",
    "few_shot_data['img_path:'].append(image)\n",
    "few_shot_data['ground_truth:'].append(fetch_ground_truth(image))\n",
    "\n",
    "\n",
    "\n",
    "# ask the prompt that has multiple examples (few shot inference)\n",
    "\n",
    "chat.embed_imgs([row[0] for row in examples], img_list)\n",
    "chat.embed_imgs([image], img_list)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\n",
    "<Img><ImageHere></Img>\n",
    "Please diagnose the image as glaucomatous or normal:\n",
    "\n",
    "Diagnosis: {examples[0][1]}\n",
    "\n",
    "<Img><ImageHere></Img>\n",
    "Please diagnose the image as glaucomatous or normal:\n",
    "\n",
    "Diagnosis: {examples[1][1]}\n",
    "\n",
    "<Img><ImageHere></Img>\n",
    "Please diagnose the image as glaucomatous or normal:\n",
    "\n",
    "Diagnosis: {examples[2][1]}\n",
    "\n",
    "<Img><ImageHere></Img>\n",
    "Please diagnose the image as glaucomatous or normal:\n",
    "\n",
    "Diagnosis:\n",
    "\"\"\"\n",
    "\n",
    "chat.ask(prompt, conv=chat_state, img_list=img_list)\n",
    "\n",
    "# have the model answer and display \n",
    "llm_message = llm_message = chat.answer(\n",
    "        conv=chat_state,\n",
    "        img_list=img_list,\n",
    "        num_beams=args.num_beams,\n",
    "        temperature=args.temperature,\n",
    "        max_new_tokens=300,\n",
    "        max_length=2000\n",
    "    )[0]\n",
    "\n",
    "few_shot_data['llm_message'].append(llm_message)\n",
    "few_shot_data['prediction:'].append(encode_diagnosis(llm_message))\n",
    "\n",
    "print(f\"Img: {image} - Prediction: {few_shot_data['prediction:'][-1]} - Ground Truth: {few_shot_data['ground_truth:'][-1]} - LLM Message: {few_shot_data['llm_message'][-1]}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n<Img>',\n",
       " \"</Img>\\nPlease diagnose the image as glaucomatous or normal:\\n\\nDiagnosis: Color fundus photography of both eyes in a 71-year old woman with severe open-angle glaucoma. Right eye (left image) demonstrates marked cupping of the optic nerve. Retinal vessels can be seen 'bayoneting' superonasal (arrow). Left eye (right image) demonstrates greater cupping than the right eye with near complete loss of neuroretinal rim and surrounding peripapillary atrophy. Retinal vessels are seen 'bayoneting' inferotemporal with brief loss of visualization upon entering the cup (arrow). Optic nerve heads are magnified in the lower right corner of each image.\\n\\n<Img>\",\n",
       " '</Img>\\nPlease diagnose the image as glaucomatous or normal:\\n\\nDiagnosis: The optic nerve shows moderate cupping but there is a prominent inferior notch. The normal sheen of the nerve fiber layer is absent in a distribution radiating temporally from this notch due to cellular loss. The notch and nerve fiber layer defect can be more easily visualized on optical coherence tomography.\\n\\n<Img>',\n",
       " '</Img>\\nPlease diagnose the image as glaucomatous or normal:\\n\\nDiagnosis: Normal Fundus. The disk has sharp margins and is normal in color, with a small central cup. Arterioles and venules have normal color, sheen, and course. Background is in normal color. The macula is enclosed by arching temporal vessels. The fovea is located by a central pit.\\n\\n<Img>',\n",
       " '</Img>\\nPlease diagnose the image as glaucomatous or normal:\\n\\nDiagnosis:\\n']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.split('<ImageHere>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
